{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "### RAG Pipelines - Data Ingestion to Vector DB ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, certifi\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from pathlib import Path\n",
    "\n",
    "os.environ[\"SSL_CERT_FILE\"] = certifi.where()\n",
    "os.environ[\"REQUESTS_CA_BUNDLE\"] = certifi.where()\n",
    "\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "# CHROMA_API_KEY = os.getenv(\"CHROMA_API_KEY\")\n",
    "# CHROMA_TENANT = os.getenv(\"CHROMA_TENANT\")\n",
    "# CHROMA_DB=os.getenv(\"RAG_DB\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "### Converting Raw PDF/File to Langchain Document Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Read all the pdf's inside directory ###\n",
    "\n",
    "all_documents = []\n",
    "def process_documents(pdf_directory):\n",
    "    \"\"\"Processes all the PDF's that are present inside the given pdf_diretory\"\"\"\n",
    "\n",
    "    pdf_dir = Path(pdf_directory)\n",
    "\n",
    "    pdf_files = list(pdf_dir.glob(\"**/*.pdf\"))\n",
    "    print(f\"Found {len(pdf_files)} PDF's to process.\")\n",
    "    print(f\"\\n\\nPDF FILES = {pdf_files}\\n\\n\")\n",
    "\n",
    "    for pdf_file in pdf_files:\n",
    "        print(f\"Processing {pdf_file.name} pdf\\n\")\n",
    "\n",
    "        try:\n",
    "\n",
    "            loader = PyPDFLoader(str(pdf_file))\n",
    "            documents = loader.load()\n",
    "\n",
    "            ## Adding information to metadata\n",
    "            for doc in documents:\n",
    "                doc.metadata['source_file'] = pdf_file.name\n",
    "                doc.metadata['file_type'] = 'pdf'\n",
    "\n",
    "\n",
    "            all_documents.extend(documents)\n",
    "            print(f\"✅Loaded {len(documents)} documents\")\n",
    "\n",
    "\n",
    "\n",
    "        except Exception as e:\n",
    "            print(\"❌Error in loading documents\")\n",
    "\n",
    "        print(f\"\\nTotal documents loaded: {len(documents)}\")\n",
    "        return documents\n",
    "\n",
    "\n",
    "# Calling the above function\n",
    "process_documents(\"../data/pdf_files/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(all_documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "### Splitting the Document structure to Smaller chunks/pieces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting Text into Chunks\n",
    "\n",
    "def split_documents(documents,chunk_size=1000,chunk_overlap=200):\n",
    "    \"\"\"Splits documents into smaller chunks for better RAG performance\"\"\"\n",
    "\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_overlap=chunk_overlap,\n",
    "        chunk_size=chunk_size,\n",
    "        length_function = len,\n",
    "        separators=[\"\\n\\n\",\"\\n\",\" \",\"\"]\n",
    "    )\n",
    "\n",
    "    split_docs= text_splitter.split_documents(documents)\n",
    "\n",
    "    print(f\"\\nSplit {len(documents)} documents into {len(split_docs)} documents successfully\")\n",
    "\n",
    "    if split_docs:\n",
    "        print(f\"\\nExample chunk: \")\n",
    "        print(f\"Content: {split_docs[0].page_content[:200]}...\")\n",
    "        print(f\"Metadata: {split_docs[0].metadata}\")\n",
    "\n",
    "\n",
    "    return split_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = split_documents(all_documents)\n",
    "print(chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "### Embedding & Vector Store DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "import uuid\n",
    "from typing import List, Dict, Any , Tuple\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingsManager:\n",
    "    \"\"\"Helps to generate embeddings by using Hugging Face SEntence transformer model.\"\"\"\n",
    "\n",
    "    def __init__(self, model_name: str = \"all-MiniLM-L6-V2\"):\n",
    "        \"\"\"\n",
    "            This init function is called as soon as the object for this class will be created.\n",
    "            This function does 3 things:\n",
    "                - Initializes the local variable 'model_name', set's it equal to the passed model_name (str)\n",
    "                - Initializes the local variable model, initially set's it to none. We will set this model attribute to the actual \n",
    "                    sentence transformer model instance.\n",
    "\n",
    "                - Calls the _load_model() function.\n",
    "        \"\"\"\n",
    "\n",
    "        self.model_name = model_name\n",
    "        self.model = None\n",
    "        self._load_model()\n",
    "\n",
    "\n",
    "    def _load_model(self):\n",
    "        \"\"\"This function is responsible to load the Hugging face sentence transformmer model.\"\"\"\n",
    "\n",
    "        try:\n",
    "\n",
    "            print(f\"Loading embedding model: {self.model_name}\")\n",
    "            self.model = SentenceTransformer(self.model_name)\n",
    "            print(f\"Model loaded successfully: Embedding dimension: {self.model.get_sentence_embedding_dimension()}\")\n",
    "\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error while loading {self.model_name}: {e}\")\n",
    "            raise\n",
    "\n",
    "\n",
    "    def generate_embeddings(self,texts: List[str]) -> np.ndarray:\n",
    "\n",
    "        \"\"\"Generate embeddings for a list of texts\n",
    "         \n",
    "          Args: \n",
    "                    texts: List of text strings\n",
    "                     \n",
    "                      \n",
    "                    Returns: \n",
    "                            numpy array of embeddings with shape (len(texts), embedding_dimensions) \"\"\"\n",
    "        \n",
    "        if not self.model:\n",
    "            raise ValueError(\"Model not loaded\")\n",
    "        \n",
    "\n",
    "        print(f\"Generate embeddings for {len(texts)} documents\")\n",
    "        embeddings = self.model.encode(texts,show_progress_bar=True)\n",
    "        print(f\"Generated embeddings with shape: {embeddings.shape}\")\n",
    "        return embeddings\n",
    "    \n",
    "\n",
    "    def get_embeddings_dimensions(self):\n",
    "        \"\"\"Get the sentence embedding dimensions for a model\"\"\"\n",
    "\n",
    "        if not self.model:\n",
    "            raise ValueError(\"Model not loaded correctly\")\n",
    "\n",
    "        return self.model.get_sentence_embedding_dimension()    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_manager = EmbeddingsManager()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "### Initializing Vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "import uuid\n",
    "from typing import List, Dict, Any , Tuple\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "\n",
    "class VectorStore:\n",
    "    \"\"\"Manages vector embeddings in a ChromaDB vector store\"\"\"\n",
    "\n",
    "    def __init__(self, collection_name: str =\"pdf_files\", persistDirectory: str = \"../data/vetor_store\"):\n",
    "        \n",
    "        self.collection_name = collection_name\n",
    "        self.persistDirectory = persistDirectory\n",
    "        self.client = None\n",
    "        self.initialize_store()\n",
    "        self.collection = self.client.get_or_create_collection(name=self.collection_name)\n",
    "\n",
    "\n",
    "    def initialize_store(self):\n",
    "\n",
    "        \"\"\"Initializes ChromaDB client and collection \"\"\"\n",
    "\n",
    "        try:\n",
    "\n",
    "            os.makedirs(self.persistDirectory, exist_ok=True) \n",
    "            # Use local persistent client instead of cloud client\n",
    "            self.client = chromadb.PersistentClient(path=self.persistDirectory)\n",
    "\n",
    "            # Get or create a collection\n",
    "            self.collection = self.client.get_or_create_collection(\n",
    "                name=self.collection_name,\n",
    "                metadata={\"description\":\"PFDF document embeddings for RAG\"}\n",
    "            )\n",
    "\n",
    "            print(f\"Vector store initialized. Collection: {self.collection_name}\")\n",
    "            print(f\"Existing documents in collection: {self.collection.count()}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error while initializing ChromaDB client: {e}\")\n",
    "            raise\n",
    "\n",
    "    def _sanitize_metadata(self, metadata: Any) -> Dict[str, Any]:\n",
    "        \"\"\"Ensure metadata is a flat dict with serializable values and no None.\n",
    "        Unsupported values are stringified. Keys with None are dropped.\n",
    "        \"\"\"\n",
    "        if not isinstance(metadata, dict):\n",
    "            return {}\n",
    "        clean: Dict[str, Any] = {}\n",
    "        for k, v in metadata.items():\n",
    "            if v is None:\n",
    "                continue\n",
    "            if isinstance(v, (str, int, float, bool)):\n",
    "                clean[k] = v\n",
    "            else:\n",
    "                try:\n",
    "                    clean[k] = str(v)\n",
    "                except Exception:\n",
    "                    # skip if completely unserializable\n",
    "                    pass\n",
    "        return clean\n",
    "\n",
    "    def add_documents(self, documents: List[Any], embeddings: np.ndarray):\n",
    "        \"\"\"\n",
    "            Add documents and their embeddings to the vector store.\n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "        try:\n",
    "\n",
    "            if(len(documents) != len(embeddings)):\n",
    "                raise ValueError(\"Documents and embeddings must have the same length\")\n",
    "\n",
    "            print(f\"Adding {len(documents)} documents to the Vector STore...\")\n",
    "\n",
    "\n",
    "            # Prepare data for ChromaDB\n",
    "\n",
    "            ids = []\n",
    "            metadatas = []\n",
    "            documents_text = []\n",
    "            embeddings_list = []\n",
    "\n",
    "            assert len(documents) == len(embeddings), (\n",
    "                f\"Mismatch: {len(documents)} documents vs {len(embeddings)} embeddings\"\n",
    "            )\n",
    "\n",
    "            for i, (doc,embedding) in enumerate(zip(documents,embeddings)):\n",
    "\n",
    "                # Generate unique ID\n",
    "                doc_id = f\"doc_{uuid.uuid4().hex[:8]}_{i}\"\n",
    "                ids.append(doc_id)\n",
    "\n",
    "                # Extract content and metadata robustly\n",
    "                if hasattr(doc, \"page_content\"):\n",
    "                    content = getattr(doc, \"page_content\", \"\")\n",
    "                    raw_metadata = getattr(doc, \"metadata\", {})\n",
    "                elif isinstance(doc, dict):\n",
    "                    content = doc.get(\"page_content\") or doc.get(\"text\") or str(doc)\n",
    "                    raw_metadata = doc.get(\"metadata\", {})\n",
    "                elif isinstance(doc, str):\n",
    "                    content = doc\n",
    "                    raw_metadata = {}\n",
    "                else:\n",
    "                    content = str(doc)\n",
    "                    raw_metadata = {}\n",
    "\n",
    "                content = content if isinstance(content, str) else str(content)\n",
    "\n",
    "                # Sanitize metadata and add our fields\n",
    "                metadata = self._sanitize_metadata(raw_metadata)\n",
    "                metadata['doc_index'] = i\n",
    "                metadata['content_length'] = len(content)\n",
    "                metadatas.append(metadata)\n",
    "\n",
    "                # Document content\n",
    "                documents_text.append(content)\n",
    "\n",
    "                # Embeddings\n",
    "                embeddings_list.append(embedding.tolist())\n",
    "\n",
    "            self.collection.add(\n",
    "                    ids = ids,\n",
    "                    documents = documents_text,\n",
    "                    embeddings = embeddings_list,\n",
    "                    metadatas = metadatas\n",
    "                )\n",
    "\n",
    "            print(f\"Successfully added {len(documents)} documents to ChromDB collection.\")\n",
    "            print(f\"Total documents in collection: {self.collection.count()}\")\n",
    "\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error while adding documents/embeddings to ChromaDB collection: {e}\")\n",
    "            raise\n",
    "\n",
    "\n",
    "\n",
    "vector_store = VectorStore()\n",
    "vector_store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Firstly, we will convert the Textual Chunks into Embeddings.\n",
    "\n",
    "texts = [getattr(doc, \"page_content\", str(doc)) for doc in chunks]\n",
    "\n",
    "# generating embeddings\n",
    "\n",
    "embeddings = embedding_manager.generate_embeddings(texts)\n",
    "\n",
    "# passing the embeddings to ChromaDB to store (use original docs so metadata is kept)\n",
    "\n",
    "vector_store.add_documents(chunks, embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "### Retriever Pipeline from VectorSTore (Pipeline 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RAGRetriever:\n",
    "    \"\"\"\n",
    "        The job of this class is to define all the member functions that help to retrieve the result from the Vector store\n",
    "        The retrieval will be done based on the user's query.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,vector_store: VectorStore, embeddings_manager: EmbeddingsManager):\n",
    "        \"\"\"\n",
    "            This init function initializes two class attributes:\n",
    "                - A Vector Store.\n",
    "                - An embeddings manager to convert text to embeddings, then do semantic search and get result from Vector Store\n",
    "        \"\"\"\n",
    "\n",
    "        self.vector_store = vector_store\n",
    "        self.embeddings_manager = embeddings_manager\n",
    "\n",
    "\n",
    "    def retrieve(self, query: str, top_k: int = 5, score_threshold: float = 0.0)-> List[Dict[str,Any]]:\n",
    "\n",
    "        \"\"\"\n",
    "            1. This function takes the user's query , the number of documents to be returned (the top matches), and a score_threshold variable.\n",
    "\n",
    "            2. As the name suggests, this function performs the retrieval from the Vector Store attribute of the RAGRetriever class, so that \n",
    "            we can provide extra context to our LLM.\n",
    "        \"\"\"\n",
    "\n",
    "        print(f\"Retrieving documents for query: {query}\")\n",
    "        print(f\"Top K : {top_k}, Score Threshold: {score_threshold}\")\n",
    "\n",
    "        #Convert the Query into an embedding (so we can match embedded query vector with embedded vector documents)\n",
    "\n",
    "        queryEmbeddings = self.embeddings_manager.generate_embeddings([query])[0]\n",
    "\n",
    "        retrieved_docs = []\n",
    "\n",
    "        #Search in Vector Store for top k matched vectors (documents)\n",
    "\n",
    "        try:\n",
    "\n",
    "            results = self.vector_store.collection.query(\n",
    "                query_embeddings=[queryEmbeddings.tolist()],\n",
    "                n_results = top_k\n",
    "                )\n",
    "            \n",
    "\n",
    "            if(results['documents'] and results['documents'][0]):\n",
    "                documents = results['documents'][0]\n",
    "                metadatas = results['metadatas'][0]\n",
    "                distances = results['distances'][0]\n",
    "                ids= results['ids'][0]\n",
    "\n",
    "\n",
    "                for i, (doc_id,document,metadata,distance) in enumerate(zip(ids,documents,metadatas,distances)):\n",
    "                    # Convert distances to Similary Score\n",
    "                    similarity_score = 1-distance \n",
    "\n",
    "                    if(similarity_score >= score_threshold):\n",
    "                        retrieved_docs.append({\n",
    "                            'id': doc_id,\n",
    "                            'content':document ,\n",
    "                            'metadata': metadata,\n",
    "                            'similarity_score': similarity_score,\n",
    "                            'distance': distance,\n",
    "                            'rank': i + 1\n",
    "                        })\n",
    "\n",
    "\n",
    "            print(f\"Retrieved Docs: {len(retrieved_docs)}\\n\")\n",
    "            return retrieved_docs\n",
    "\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"An error ocurred while performing Retrieval of documents: {e}\")\n",
    "            \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_retriever = RAGRetriever(vector_store=vector_store, embeddings_manager=embedding_manager)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "### Integrating the Vector DB context with LLM Output (Augmented Generation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "groq_api_key = os.getenv(\"GROQ_API_KEY\")\n",
    "\n",
    "\n",
    "### Initializing the Groq LLM\n",
    "llm = ChatGroq(api_key=groq_api_key, model_name=\"llama-3.1-8b-instant\", temperature=0.1, max_tokens=1024)\n",
    "\n",
    "\n",
    "### simple RAG function.\n",
    "\n",
    "def simple_rag_function(query,retriever = rag_retriever,llm = llm,top_k=3):\n",
    "\n",
    "    ###Retrieving the context\n",
    "    retrievedContext = rag_retriever.retrieve(query=query,top_k=top_k)\n",
    "    context = \"\\n\\n\".join([doc['content'] for doc in retrievedContext]) if retrievedContext else \"\"\n",
    "\n",
    "\n",
    "    if not context:\n",
    "        return \"No Relevant context found to the query.\"\n",
    "    \n",
    "\n",
    "    ###Generating the Final Answer using Groq LLM.\n",
    "\n",
    "    prompt = f\"\"\"Use the following context to answer the question concisely.\n",
    "\n",
    "        Context:\n",
    "        {context}\n",
    "\n",
    "        Question: {query}\n",
    "\n",
    "        Answer:\"\"\" \n",
    "    \n",
    "\n",
    "    llm_response = llm.invoke([prompt.format(context=context,query=query)])\n",
    "\n",
    "    return llm_response.content\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = simple_rag_function(query=\"\")\n",
    "print(answer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
