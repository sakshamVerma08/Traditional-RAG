# ğŸ§  RAG Evaluation Dataset

This repository contains multiple **PDF documents** and **prompt sets** designed to evaluate the quality, accuracy, and grounding of a Retrieval-Augmented Generation (RAG) system.

---

## ğŸ“ Folder Structure

data/
â”‚
â”œâ”€â”€ pdf_files/
â”‚ â”œâ”€â”€ aws_well_architected_framework.pdf
â”‚ â”œâ”€â”€ aws_security_pillar.pdf
â”‚ â”œâ”€â”€ apple_annual_report_2024.pdf
â”‚ â”œâ”€â”€ nasa_artemis_overview.pdf
â”‚ â””â”€â”€ art_of_war.pdf
â”‚
â”œâ”€â”€ prompts/
â”‚ â”œâ”€â”€ aws_prompts.jsonl
â”‚ â”œâ”€â”€ apple_prompts.jsonl
â”‚ â”œâ”€â”€ nasa_prompts.jsonl
â”‚ â”œâ”€â”€ art_of_war_prompts.jsonl
â”‚ â””â”€â”€ generic_grounding_tests.jsonl
â”‚
â””â”€â”€ README.md



---

## âš™ï¸ Setup Instructions

### 1. Prepare Environment
Use any RAG framework (LangChain, LlamaIndex, Haystack, etc.).

Example (LangChain + ChromaDB):

```python
from langchain_community.document_loaders import PyPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_chroma import Chroma
from langchain.embeddings import OpenAIEmbeddings

loader = PyPDFLoader("docs/aws_well_architected_framework.pdf")
docs = loader.load()

splitter = RecursiveCharacterTextSplitter(chunk_size=800, chunk_overlap=100)
chunks = splitter.split_documents(docs)

vectorstore = Chroma.from_documents(chunks, OpenAIEmbeddings())
